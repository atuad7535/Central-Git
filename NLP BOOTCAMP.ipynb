{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6861c44c",
   "metadata": {},
   "source": [
    "# SPACY\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7fb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u'Tesla is looking at buying U.S. startup for $6 million') #unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a7441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN 96\n",
      "is AUX 87\n",
      "looking VERB 100\n",
      "at ADP 85\n",
      "buying VERB 100\n",
      "U.S. PROPN 96\n",
      "startup VERB 100\n",
      "for ADP 85\n",
      "$ SYM 99\n",
      "6 NUM 93\n",
      "million NUM 93\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.pos)#pos_ gives the part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a092",
   "metadata": {},
   "source": [
    " Pipeline\n",
    "\n",
    "When we run nlp, our text enters a processing pipeline that first breaks down the text and then performs a series of operations to tag, parse and describe the data. Image source:https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8cd445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x2357c0c6880>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2357c0c6b20>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2357bfd1eb0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2357c269100>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x2357c25ff80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2357bfd1dd0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a5e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deacce76",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "\n",
    "The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37cc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Tesla isn't looking into startups anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792af1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN\n",
      "is AUX\n",
      "n't PART\n",
      "looking VERB\n",
      "into ADP\n",
      "startups NOUN\n",
      "anymore ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b89ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a3073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(doc2[3].text)\n",
    "print(doc2[3].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7193f",
   "metadata": {},
   "source": [
    "Spans\n",
    "\n",
    "Large Doc objects can be hard to work with at times. A span is a slice of Doc object in the form Doc[start:stop]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc541e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1342b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c7ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f5a85",
   "metadata": {},
   "source": [
    "Sentences\n",
    "\n",
    "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through Doc.sents.We can write our own segmentation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2093fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7ccbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3efd7c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3e90d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[8].is_sent_start #it means that sentence doesn't starts from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f2220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae0a0de",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "The first step in creating a Doc object is to break down the incoming text into component pieces or \"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6b5930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to L.A.!\"\n"
     ]
    }
   ],
   "source": [
    "#string that includes opening and closing quotation marks\n",
    "mystring = '\"We\\'re moving to L.A.!\"'\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba765fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" | We | 're | moving | to | L.A. | ! | \" | "
     ]
    }
   ],
   "source": [
    "# Create a Doc object and explore tokens\n",
    "doc = nlp(mystring)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b75a3",
   "metadata": {},
   "source": [
    "Prefixes, Suffixes and InfixesÂ¶\n",
    "\n",
    "spaCy will isolate punctuation that does not form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff69016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We      PRON\n",
      "'re      AUX\n",
      "here      ADV\n",
      "to      PART\n",
      "help      VERB\n",
      "!      PUNCT\n",
      "Send      VERB\n",
      "snail      NOUN\n",
      "-      PUNCT\n",
      "mail      NOUN\n",
      ",      PUNCT\n",
      "email      NOUN\n",
      "support@oursite.com      X\n",
      "or      CCONJ\n",
      "visit      VERB\n",
      "us      PRON\n",
      "at      ADP\n",
      "http://www.oursite.com      X\n",
      "!      PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "\n",
    "for t in doc2:\n",
    "    print(t,\"    \",t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153e4035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "NYC\n",
      "cab\n",
      "ride\n",
      "costs\n",
      "$\n",
      "10.30\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
    "\n",
    "for t in doc3:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae06cc0",
   "metadata": {},
   "source": [
    "Exceptions\n",
    "\n",
    "Punctuation that exists as part of a known abbreviation will be kept as part of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebae626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "visit\n",
      "St.\n",
      "Louis\n",
      "in\n",
      "the\n",
      "U.S.\n",
      "next\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
    "\n",
    "for t in doc4:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67fcbfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc4) #count of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067907da",
   "metadata": {},
   "source": [
    "Counting Vocab Entries\n",
    "\n",
    "Vocab objects contain a full library of items!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab827938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc4.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fd5ea",
   "metadata": {},
   "source": [
    "Tokens can be retrieved by index position and slice\n",
    "\n",
    "Doc objects can be thought of as lists of token objects. As such, individual tokens can be retrieved by index position, and spans of tokens can be retrieved through slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c5c0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "better"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp(u'It is better to give than to receive.') #No reassingments can haapen when doc is created\n",
    "\n",
    "# Retrieve the third token:\n",
    "doc5[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c69e586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "better to give"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve three tokens from the middle:\n",
    "doc5[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbe5260f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "than to receive."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the last four tokens:\n",
    "doc5[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f184d33",
   "metadata": {},
   "source": [
    "Named Entities\n",
    "\n",
    "Going a step beyond tokens, named entities add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the ents property of a Doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb49c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | "
     ]
    }
   ],
   "source": [
    "doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26f44ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple - ORG - Companies, agencies, institutions, etc.\n",
      "Hong Kong - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "532f9044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc8.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3256f",
   "metadata": {},
   "source": [
    "Noun Chunks  https://spacy.io/usage/linguistic-features#noun-chunks\n",
    "\n",
    "Similar to Doc.ents, Doc.noun_chunks are another object property. Noun chunks are \"base noun phrases\" â flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1741b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "177918d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
    "\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ceb44",
   "metadata": {},
   "source": [
    "Built-in Visualizers\n",
    "\n",
    "spaCy includes a built-in visualization tool called displaCy. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When you export your notebook, the visualizations will be included as HTML.\n",
    "\n",
    "For more info visit https://spacy.io/usage/visualizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "427870d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e680df1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"76db21486d564d54904d2501ad5218e6-0\" class=\"displacy\" width=\"1250\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,102.0 240.0,102.0 240.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L62,192.0 78,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-1\" stroke-width=\"2px\" d=\"M170,202.0 C170,152.0 235.0,152.0 235.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,204.0 L162,192.0 178,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-2\" stroke-width=\"2px\" d=\"M370,202.0 C370,152.0 435.0,152.0 435.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,204.0 L362,192.0 378,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-3\" stroke-width=\"2px\" d=\"M270,202.0 C270,102.0 440.0,102.0 440.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M440.0,204.0 L448.0,192.0 432.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-4\" stroke-width=\"2px\" d=\"M570,202.0 C570,102.0 740.0,102.0 740.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,204.0 L562,192.0 578,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-5\" stroke-width=\"2px\" d=\"M670,202.0 C670,152.0 735.0,152.0 735.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,204.0 L662,192.0 678,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-6\" stroke-width=\"2px\" d=\"M470,202.0 C470,52.0 745.0,52.0 745.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,204.0 L753.0,192.0 737.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-7\" stroke-width=\"2px\" d=\"M470,202.0 C470,2.0 850.0,2.0 850.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,204.0 L858.0,192.0 842.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-8\" stroke-width=\"2px\" d=\"M970,202.0 C970,102.0 1140.0,102.0 1140.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,204.0 L962,192.0 978,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-9\" stroke-width=\"2px\" d=\"M1070,202.0 C1070,152.0 1135.0,152.0 1135.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1070,204.0 L1062,192.0 1078,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-76db21486d564d54904d2501ad5218e6-0-10\" stroke-width=\"2px\" d=\"M870,202.0 C870,52.0 1145.0,52.0 1145.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-76db21486d564d54904d2501ad5218e6-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1145.0,204.0 L1153.0,192.0 1137.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0997614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n",
    "#returns the entity with its type\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc9d4dee",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f9044",
   "metadata": {},
   "source": [
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the stem for [boat, boater, boating, boats].\n",
    "\n",
    "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization. For those interested, there's some background on this decision here. We discuss the virtues of lemmatization in the next section.\n",
    "\n",
    "Instead, we'll use another popular NLP tool called nltk, which stands for Natural Language Toolkit. For more information on nltk visit https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63008dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa7cddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c1acd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b45c5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b01d5c",
   "metadata": {},
   "source": [
    "Also, the adverbs \"easily\" and \"fairly\" are stemmed to the unusual root \"easili\" and \"fairli\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5cb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b86d943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bafd7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06d0dbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c40e66",
   "metadata": {},
   "source": [
    "Stemming has its drawbacks. If given the token saw, stemming might always return saw, whereas lemmatization would likely return either see or saw depending on whether the use of the token was as a verb or a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16194b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "phrase = 'I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af76ad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "for word in phrase.split():\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722a6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f8acfe",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e37c2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b6bc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text): #formatting text to see a proper output\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b069d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95f5f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That         PRON   4380130941430378203    that\n",
      "'s           AUX    10382539506755952630   be\n",
      "an           DET    15099054000809333061   an\n",
      "enormous     ADJ    17917224542039855524   enormous\n",
      "automobile   NOUN   7211811266693931283    automobile\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"That's an enormous automobile\")\n",
    "\n",
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bbab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39fb83cc",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "Words like \"a\" and \"the\" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these stop words, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 326 English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae36393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eight', 'three', 'âd', 'just', 'due', 'sometime', 'thru', 'put', 'twenty', 'over', 'top', 'nor', 'before', 'seeming', 'until', 'whereby', 'nobody', 'so', 'thereby', 'least', 'move', 'everywhere', 'why', 'again', 'otherwise', 'eleven', 'sixty', 'among', 'down', 'therein', 'ten', 'onto', 'make', 'other', 'indeed', 'first', 'using', 'somewhere', 'unless', 'you', 'had', 'whom', 'amongst', 'even', 'could', 'always', 'whenever', 'up', 'everything', 'get', 'hereafter', 'nevertheless', \"'ll\", 'anyhow', 'several', 'doing', 'together', 'everyone', 'âm', 'it', 'afterwards', 'thereafter', 'these', 'under', 'âre', 'me', \"n't\", 'however', 'such', 'being', 'full', 'him', 'two', 'would', 'a', 'their', 'which', 'but', 'sometimes', 'now', 'the', 'above', 'mine', \"'s\", 'perhaps', 'go', 'some', 'hence', 'this', 'those', 'at', 'beforehand', 'his', 'whatever', 'whether', 'please', 'whole', 'also', 'most', 'our', \"'m\", 'therefore', 'towards', 'made', 'whither', 'six', 'behind', 'when', 'herself', 'serious', 'less', 'somehow', 'there', 'did', 'becomes', 'became', 'should', 'wherein', 'âll', 'may', 'moreover', 'us', 'more', 'an', 'enough', 'from', 'âm', 'has', 'hers', 'mostly', 'only', 'take', 'below', 'various', 'âre', 'fifteen', 'wherever', 'âve', 'as', 'via', 'keep', 'yet', 'be', 'give', 'here', 'call', 'to', 'where', 'through', 'become', 'they', 'never', 'say', 'âll', 'its', 'in', 'another', 'along', 'beyond', 'either', 'same', 'last', 'between', 'her', 'each', 'meanwhile', 'bottom', 'yourselves', 'one', 'nine', 'might', 'whose', 'others', 'except', 'since', 'anything', 'are', 'every', 'while', 'âs', 'back', 'what', 'show', 'were', 'have', 'then', 'against', 'still', 'how', 'rather', 'without', 'we', 'for', 'done', 'nât', 'noone', 'whereupon', 'nowhere', 'cannot', 'latterly', 'ours', 'of', 'becoming', 'hundred', 'per', 'really', 'toward', 'quite', 'used', 'seems', 'part', 'into', 'nât', 'been', 'not', 'with', 'was', \"'ve\", 'âd', 'besides', 'once', 'none', 'both', 'four', 'empty', 'that', 'see', 'whereas', 'around', 'within', 'whence', 'does', 'name', 'own', 'third', 'ca', 'five', 'though', 'and', 'often', 'them', 'after', 'whereafter', 'on', \"'re\", 'who', 'itself', 'latter', 'across', 'few', 'during', 'he', 'someone', 'do', 'well', 'than', 'or', 'fifty', 'something', 'she', 'myself', 'any', 'must', 'much', 'themselves', 'neither', 'anywhere', 're', 'anyone', 'next', 'himself', 'herein', 'can', 'already', 'âve', 'ourselves', 'because', 'elsewhere', 'formerly', 'very', 'hereby', 'anyway', 'thence', 'about', 'alone', 'regarding', 'am', 'forty', 'further', 'off', 'ever', 'out', 'by', \"'d\", 'seem', 'namely', 'although', 'upon', 'former', 'thereupon', 'seemed', 'yourself', 'side', 'if', 'yours', 'hereupon', 'else', 'will', 'no', 'many', 'too', 'nothing', 'thus', 'i', 'whoever', 'amount', 'beside', 'almost', 'front', 'is', 'âs', 'my', 'your', 'all', 'throughout', 'twelve'}\n"
     ]
    }
   ],
   "source": [
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe620dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "058f7a9a",
   "metadata": {},
   "source": [
    "To add a stop word\n",
    "\n",
    "There may be times when you wish to add a stop word to the default set. Perhaps you decide that 'btw' (common shorthand for \"by the way\") should be considered a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "181771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beedc7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa445472",
   "metadata": {},
   "source": [
    "When adding stop words, always use lowercase. Lexemes are converted to lowercase before being added to vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133657c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8148bff9",
   "metadata": {},
   "source": [
    "To remove a stop word\n",
    "\n",
    "Alternatively, you may decide that 'beyond' should not be considered a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23e638d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d64616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c9c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcb39fe",
   "metadata": {},
   "source": [
    "# Vocabulary and Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fcf68",
   "metadata": {},
   "source": [
    "Rule-based Matching https://spacy.io/usage/linguistic-features#section-rule-based-matching\n",
    "\n",
    "spaCy offers a rule-matching tool called Matcher that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f8c93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8ddf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}] #SolarPower\n",
    "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}] #Solar power\n",
    "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}] #Solar-power\n",
    "\n",
    "matcher.add('SolarPower', [pattern1, pattern2, pattern3],on_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "395bcbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d532b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a3e6c",
   "metadata": {},
   "source": [
    "matcher returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc770321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f012fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4918cef",
   "metadata": {},
   "source": [
    "PhraseMatcher\n",
    "\n",
    "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into matcher instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "523ebd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "914500a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\hp\\\\Desktop\\\\UPDATED_NLP_COURSE\\\\UPDATED_NLP_COURSE\\\\TextFiles\\\\reaganomics.txt') as f:\n",
    "    doc3 = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cb1393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of match phrases:\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac8f2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns=[]\n",
    "for text in phrase_list:\n",
    "    phrase_patterns.append((nlp(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4026b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass each Doc object into matcher (note the use of the asterisk!):\n",
    "matcher.add('VoodooEconomics', None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77dfd161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3473369816841043438, 41, 45),\n",
       " (3473369816841043438, 49, 53),\n",
       " (3473369816841043438, 54, 56),\n",
       " (3473369816841043438, 61, 65),\n",
       " (3473369816841043438, 673, 677),\n",
       " (3473369816841043438, 2987, 2991)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a list of matches:\n",
    "matches = matcher(doc3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afbce2e8",
   "metadata": {},
   "source": [
    "Viewing Matches\n",
    "\n",
    "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fdd4ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ebd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d448de4",
   "metadata": {},
   "source": [
    "Another way is to first apply the sentencizer to the Doc, then iterate through the sentences to the match point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "481765a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[]\n",
    "for sent in doc3.sents:\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29a4f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35\n"
     ]
    }
   ],
   "source": [
    "print(sents[0].start, sents[0].end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecb4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a09f6b",
   "metadata": {},
   "source": [
    "# Part of Speech Basics\n",
    "\n",
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10da770",
   "metadata": {},
   "source": [
    "View token tags\n",
    "\n",
    "To view the coarse POS tag use token.pos_\n",
    "\n",
    "To view the fine-grained tag use token.tag_\n",
    "\n",
    "To view the description of either type of tag use spacy.explain(tag)\n",
    "\n",
    "Note that `token.pos` and `token.tag` return integer hash values; by adding the underscores we get the text equivalent that lives in **doc.vocab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f5a4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Doc object\n",
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2df7bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumped\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(doc[4])\n",
    "print(doc[4].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bbde902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        DET      DT     determiner\n",
      "quick      ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "brown      ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "fox        NOUN     NN     noun, singular or mass\n",
      "jumped     VERB     VBD    verb, past tense\n",
      "over       ADP      IN     conjunction, subordinating or preposition\n",
      "the        DET      DT     determiner\n",
      "lazy       ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "dog        NOUN     NN     noun, singular or mass\n",
      "'s         PART     POS    possessive ending\n",
      "back       NOUN     NN     noun, singular or mass\n",
      ".          PUNCT    .      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccdb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4940248f",
   "metadata": {},
   "source": [
    "Counting POS Tags\n",
    "\n",
    "The Doc.count_by() method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object. Keys in the dictionary are the integer values of the given attribute ID, and values are the frequency. Counts of zero are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95833b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 2, 84: 3, 92: 3, 100: 1, 85: 1, 94: 1, 97: 1}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n",
    "\n",
    "# Count the frequencies of different coarse-grained POS tags:\n",
    "POS_counts = doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf90ef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[90].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d319db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[84].text #means we have 3 adjectives are in the doc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2de01eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84. ADJ  : 3\n",
      "85. ADP  : 1\n",
      "90. DET  : 2\n",
      "92. NOUN : 3\n",
      "94. PART : 1\n",
      "97. PUNCT: 1\n",
      "100. VERB : 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(POS_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc3dc556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74. POS : 1\n",
      "1292078113972184607. IN  : 1\n",
      "10554686591937588953. JJ  : 3\n",
      "12646065887601541794. .   : 1\n",
      "15267657372422890137. DT  : 2\n",
      "15308085513773655218. NN  : 3\n",
      "17109001835818727656. VBD : 1\n"
     ]
    }
   ],
   "source": [
    "# Count the different fine-grained tags:\n",
    "TAG_counts = doc.count_by(spacy.attrs.TAG)\n",
    "\n",
    "for k,v in sorted(TAG_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e596355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b51add57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402.  amod: 3\n",
      "415.   det: 2\n",
      "429. nsubj: 1\n",
      "439.  pobj: 1\n",
      "440.  poss: 1\n",
      "443.  prep: 1\n",
      "445. punct: 1\n",
      "8110129090154140942.  case: 1\n",
      "8206900633647566924.  ROOT: 1\n"
     ]
    }
   ],
   "source": [
    "# Count the different dependencies:\n",
    "DEP_counts = doc.count_by(spacy.attrs.DEP)\n",
    "\n",
    "for k,v in sorted(DEP_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:>{5}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73482b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "291a8500",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "spaCy has an 'ner' pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the ents property of a Doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b36d9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text+\" - \"+ent.label_ + \" - \"+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print(\"No entities found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30d39d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington - GPE - Countries, cities, states\n",
      "DC - GPE - Countries, cities, states\n",
      "next May - DATE - Absolute or relative dates or periods\n",
      "the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b7a11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entities found\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(u'Hi, how are you?')\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "afe4064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c9cf61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Get the hash value of the ORG entity label\n",
    "ORG = doc.vocab.strings[u'ORG'] \n",
    "ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66e77e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Span for the new entity\n",
    "new_ent = Span(doc, 0,1, label=ORG)\n",
    "new_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40f2f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the entity to the existing Doc object\n",
    "doc.ents = list(doc.ents) + [new_ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48628db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla - ORG - Companies, agencies, institutions, etc.\n",
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8e2c12",
   "metadata": {},
   "source": [
    "Adding Named Entities to All Matching Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41924558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entities found\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Our company plans to introduce a new vacuum cleaner. '\n",
    "          u'If successful, the vacuum-cleaner will be our best in show.')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b52b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3fb412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1b5ac137",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list=['vacuum cleaner','vacuum-cleaner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b01c225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns=[]\n",
    "for text in phrase_list:\n",
    "    phrase_patterns.append(nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f68be6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('newproduct',None,*phrase_patterns) #adding multiple terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d515177",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches=matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cab939b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2689272359382549672, 7, 9), (2689272359382549672, 14, 17)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b9f61401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "# Here we create Spans from each match, and create named entities from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07920637",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = doc.vocab.strings[u'PRODUCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ae5d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ents = [Span(doc, match[1],match[2],label=PROD) for match in found_matches]\n",
    "\n",
    "doc.ents = list(doc.ents) + new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2707da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n",
      "vacuum-cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea653958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27fade1",
   "metadata": {},
   "source": [
    "Counting Entities\n",
    "\n",
    "While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96112940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.50 - MONEY - Monetary values, including unit\n",
      "five dollars - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b572cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for ent in doc.ents:\n",
    "    if ent.label_=='MONEY':\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97640215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad42c706",
   "metadata": {},
   "source": [
    "# Visualizing Named Entities\n",
    "\n",
    "Besides viewing Part of Speech dependencies with style='dep', displaCy offers a style='ent' visualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ffdf6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "88f4d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". By contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 7 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walkman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " music players.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n",
    "         u'By contrast, Sony sold only 7 thousand Walkman music players.')\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c314c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n",
    "         u'By contrast, my kids sold a lot of lemonade.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07eeb2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">By contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 7 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walkman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " music players.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    displacy.render(nlp(sent.text),style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed2790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5118f0a4",
   "metadata": {},
   "source": [
    "Viewing Specific Entities\n",
    "\n",
    "You can pass a list of entity types to restrict the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73ef8f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over the last quarter \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold nearly 20 thousand \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of $6 million. By contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold only 7 thousand Walkman music players.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = {'ents': ['ORG', 'PRODUCT']}\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a384599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffefbaf7",
   "metadata": {},
   "source": [
    "Customizing Colors and Effects\n",
    "\n",
    "You can also pass background color and gradient options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db3b7190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over the last quarter \n",
       "<mark class=\"entity\" style=\"background: red; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold nearly 20 thousand \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of $6 million. By contrast, \n",
       "<mark class=\"entity\" style=\"background: red; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold only 7 thousand Walkman music players.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = {'ORG': 'red', 'PRODUCT': 'green'}\n",
    "\n",
    "options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb26ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c34c94",
   "metadata": {},
   "source": [
    "# Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is the problem of dividing a string of written language into its component sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74f88468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Spacy Basics:\n",
    "doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "503e80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: #doc.sents is a generator\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116af4ac",
   "metadata": {},
   "source": [
    "Doc.sents is a generator\n",
    "\n",
    "It is important to note that doc.sents is a generator. That is, a Doc is not segmented until doc.sents is called. This means that, where you could print the second Doc token with print(doc[1]), you can't call the \"second Doc sentence\" with print(doc.sents[1]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1c12063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right; leadership is doing the right things.\"\n",
      "-Peter Drucker\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e2b121f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Management is doing things right; leadership is doing the right things.\" -Peter"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[0:-1] #its taking only till peter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c8377",
   "metadata": {},
   "source": [
    "ADD A NEW RULE TO THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b90c841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding new rule based on \";\"\n",
    "from spacy.language import Language #This has to be done in spacy 3.0+\n",
    "@Language.component(\"component\")\n",
    "def set_custom(doc):\n",
    "\n",
    "    for token in doc[:-1]:#we are doing cuz :1 leaves the last word\n",
    "        if token.text==';':\n",
    "            doc[token.i+1].is_sent_start=True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ac759f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom(doc)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"component\", before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ebd6e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'component',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39d31a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right;\n",
      "leadership is doing the right things.\"\n",
      "-Peter Drucker\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84079a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9714c8e",
   "metadata": {},
   "source": [
    "Changing the Rules\n",
    "\n",
    "In some cases we want to replace spaCy's default sentencizer with our own set of rules. In this section we'll see how the default sentencizer breaks on periods. We'll then replace this behavior with a sentencizer that breaks on linebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8223c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # reset to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2cd10260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence. This is another.\n",
      "\n",
      "This is a \n",
      "third sentence.\n"
     ]
    }
   ],
   "source": [
    "mystring = u\"This is a sentence. This is another.\\n\\nThis is a \\nthird sentence.\"\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95c4c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another.\n",
      "\n",
      "\n",
      "This is a \n",
      "third sentence.\n"
     ]
    }
   ],
   "source": [
    "# SPACY DEFAULT BEHAVIOR:\n",
    "doc = nlp(mystring)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent) #here we can have issue if we want to have \\n to be new seegmenter of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c5bf4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to see on sentence segmentizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb8d59db",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fd314",
   "metadata": {},
   "source": [
    "Feature Extraction from Text\n",
    "\n",
    "In the Scikit-learn Primer lecture we applied a simple SVC classification model to the SMSSpamCollection dataset. We tried to predict the ham/spam label based on message length and punctuation counts. In this section we'll actually look at the text of each message and try to perform a classification based on content. We'll take advantage of some of scikit-learn's https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23ebad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c84c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"E:/spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9d71b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f2078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Length']=df['Message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e077d82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  Length\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1      ham                      Ok lar... Joking wif u oni...      29\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3      ham  U dun say so early hor... U c already then say...      49\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5ae5a48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "Length      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da89a090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eaf1d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Message']  # this time we want to look at the text\n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f52a9",
   "metadata": {},
   "source": [
    "Scikit-learn's CountVectorizer\n",
    "\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2e867257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a72363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf94d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8b7638aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7081)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) \n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3fbb61cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24d603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "627d106f",
   "metadata": {},
   "source": [
    "We can covert all above steps into a single pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250462c9",
   "metadata": {},
   "source": [
    "Build a Pipeline\n",
    "\n",
    "Remember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0fb85e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3eb2af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4e01956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3601d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e07f6ef9",
   "metadata": {},
   "source": [
    "Test the classifier and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1294cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ddad82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1586    7]\n",
      " [  12  234]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "988432b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1593\n",
      "        spam       0.97      0.95      0.96       246\n",
      "\n",
      "    accuracy                           0.99      1839\n",
      "   macro avg       0.98      0.97      0.98      1839\n",
      "weighted avg       0.99      0.99      0.99      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2e9b39ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.989668297988037"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f7f94c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Hi Atul, how are you?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fbff0aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Hi Atul, how r u?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693345a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f223570",
   "metadata": {},
   "source": [
    "# Semantics and Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f89ef3",
   "metadata": {},
   "source": [
    "Word Vectors\n",
    "\n",
    "Word vectors - also called word embeddings - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive context. As mentioned above, the word vector for \"lion\" will be closer in value to \"cat\" than to \"dandelion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e4cfae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "94060c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8963e-01, -4.0309e-01,  3.5350e-01, -4.7907e-01, -4.3311e-01,\n",
       "        2.3857e-01,  2.6962e-01,  6.4332e-02,  3.0767e-01,  1.3712e+00,\n",
       "       -3.7582e-01, -2.2713e-01, -3.5657e-01, -2.5355e-01,  1.7543e-02,\n",
       "        3.3962e-01,  7.4723e-02,  5.1226e-01, -3.9759e-01,  5.1333e-03,\n",
       "       -3.0929e-01,  4.8911e-02, -1.8610e-01, -4.1702e-01, -8.1639e-01,\n",
       "       -1.6908e-01, -2.6246e-01, -1.5983e-02,  1.2479e-01, -3.7276e-02,\n",
       "       -5.7125e-01, -1.6296e-01,  1.2376e-01, -5.5464e-02,  1.3244e-01,\n",
       "        2.7519e-02,  1.2592e-01, -3.2722e-01, -4.9165e-01, -3.5559e-01,\n",
       "       -3.0630e-01,  6.1185e-02, -1.6932e-01, -6.2405e-02,  6.5763e-01,\n",
       "       -2.7925e-01, -3.0450e-03, -2.2400e-02, -2.8015e-01, -2.1975e-01,\n",
       "       -4.3188e-01,  3.9864e-02, -2.2102e-01, -4.2693e-02,  5.2748e-02,\n",
       "        2.8726e-01,  1.2315e-01, -2.8662e-02,  7.8294e-02,  4.6754e-01,\n",
       "       -2.4589e-01, -1.1064e-01,  7.2250e-02, -9.4980e-02, -2.7548e-01,\n",
       "       -5.4097e-01,  1.2823e-01, -8.2408e-02,  3.1035e-01, -6.3394e-02,\n",
       "       -7.3755e-01, -5.4992e-01,  9.9999e-02, -2.0758e-01, -3.9674e-02,\n",
       "        2.0664e-01, -9.7557e-02, -3.7092e-01,  2.7901e-01, -6.2218e-01,\n",
       "       -1.0280e-01,  2.3271e-01,  4.3838e-01,  3.2445e-02, -2.9866e-01,\n",
       "       -7.3611e-02,  7.1594e-01,  1.4241e-01,  2.7770e-01, -3.9892e-01,\n",
       "        3.6656e-02,  1.5759e-01,  8.2014e-02, -5.7343e-01,  3.5457e-01,\n",
       "        2.2491e-01, -6.2699e-01, -8.8106e-02,  2.4361e-01,  3.8533e-01,\n",
       "       -1.4083e-01,  1.7691e-01,  7.0897e-02,  1.7951e-01, -4.5907e-01,\n",
       "       -8.2120e-01, -2.6631e-02,  6.2549e-02,  4.2415e-01, -8.9630e-02,\n",
       "       -2.4654e-01,  1.4156e-01,  4.0187e-01, -4.1232e-01,  8.4516e-02,\n",
       "       -1.0626e-01,  7.3145e-01,  1.9217e-01,  1.4240e-01,  2.8511e-01,\n",
       "       -2.9454e-01, -2.1948e-01,  9.0460e-01, -1.9098e-01, -1.0340e+00,\n",
       "       -1.5754e-01, -1.1964e-01,  4.9888e-01, -1.0624e+00, -3.2820e-01,\n",
       "       -1.1232e-02, -7.9482e-01,  3.7275e-01, -6.8710e-03, -2.5772e-01,\n",
       "       -4.7005e-01, -4.1387e-01, -6.4089e-02, -2.8033e-01, -4.0778e-02,\n",
       "       -2.4866e+00,  6.2494e-03, -1.0210e-02,  1.2752e-01,  3.4965e-01,\n",
       "       -1.2571e-01,  3.1570e-01,  4.1926e-01,  2.0056e-01, -5.5984e-01,\n",
       "       -2.2801e-01,  1.2012e-01, -2.0518e-03, -8.9764e-02, -8.0373e-02,\n",
       "        1.1969e-02, -2.6978e-01,  3.4829e-01,  7.3664e-03, -1.1137e-01,\n",
       "        6.3410e-01,  3.8449e-01, -6.2248e-01,  4.1145e-02,  2.5922e-01,\n",
       "        6.5811e-01, -4.9548e-01, -1.3030e-01, -3.8279e-01,  1.1156e-01,\n",
       "       -4.3085e-01,  3.4473e-01,  2.7109e-02, -2.5108e-01, -2.8011e-01,\n",
       "        2.1662e-01,  3.2660e-01,  5.5895e-02,  7.6077e-02, -5.2480e-02,\n",
       "        4.5928e-02, -2.5266e-01,  5.2845e-01, -1.3145e-01, -1.2453e-01,\n",
       "        4.0556e-01,  3.1877e-01,  2.4415e-02, -2.2620e-01, -6.1960e-01,\n",
       "       -4.0886e-01, -3.5534e-02, -5.5123e-03,  2.3438e-01,  8.7854e-01,\n",
       "       -2.5161e-01,  4.0600e-01, -4.4284e-01,  3.4934e-01, -5.6429e-01,\n",
       "       -2.3676e-01,  6.2199e-01, -2.8175e-01,  4.2024e-01,  1.0043e-01,\n",
       "       -1.4720e-01,  4.9593e-01, -3.5850e-01, -1.3998e-01, -2.7494e-01,\n",
       "        2.3827e-01,  5.7268e-01,  7.9025e-02,  1.7872e-02, -2.1829e-01,\n",
       "        5.5050e-02, -5.4200e-01,  1.6788e-01,  3.9065e-01,  3.0209e-01,\n",
       "        2.3040e-01, -3.9351e-02, -2.1078e-01, -2.7224e-01,  1.6907e-01,\n",
       "        5.4819e-01,  9.4888e-02,  7.9798e-01, -6.6158e-02,  1.9844e-01,\n",
       "        2.0307e-01,  4.4808e-02, -1.0240e-01, -6.9909e-02, -3.6756e-02,\n",
       "        9.5159e-02, -2.7830e-01, -1.0597e-01, -1.6276e-01, -1.8211e-01,\n",
       "       -3.1897e-01, -2.1633e-01,  1.4994e-01, -7.2057e-02,  2.2264e-01,\n",
       "       -4.5551e-01,  3.0341e-01,  1.8431e-01,  2.1681e-01, -3.1940e-01,\n",
       "        2.6426e-01,  5.8106e-01,  5.4635e-02,  6.3238e-01,  4.3169e-01,\n",
       "        9.0343e-02,  1.9494e-01,  3.5483e-01, -2.0706e-02, -7.3117e-01,\n",
       "        1.2941e-01,  1.7418e-01, -1.5065e-01,  5.3355e-02,  4.4794e-02,\n",
       "       -1.6600e-01,  2.2007e-01, -5.3970e-01, -2.4968e-01, -2.6464e-01,\n",
       "       -5.5515e-01,  5.8242e-01,  2.2295e-01,  2.4433e-01,  4.5275e-01,\n",
       "        3.4693e-01,  1.2255e-01, -3.9059e-02, -3.2749e-01, -2.7891e-01,\n",
       "        1.3766e-01,  3.8392e-01,  1.0543e-03, -1.0242e-02,  4.9205e-01,\n",
       "       -1.7922e-01,  4.1215e-02,  1.3547e-01, -2.0598e-01, -2.3194e-01,\n",
       "       -7.7701e-01, -3.8237e-01, -7.6383e-01,  1.9418e-01, -1.5441e-01,\n",
       "        8.9740e-01,  3.0626e-01,  4.0376e-01,  2.1738e-01, -3.8050e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'lion').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c4aba",
   "metadata": {},
   "source": [
    "What's interesting is that Doc and Span objects themselves have vectors, derived from the averages of individual token vectors.\n",
    "This makes it possible to compare similarities between whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62892026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy dogs.')\n",
    "\n",
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca525c05",
   "metadata": {},
   "source": [
    "Identifying similar vectors\n",
    "\n",
    "The best way to expose vector relationships is through the .similarity() method of Doc tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "234078e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.5265437364578247\n",
      "lion pet 0.39923766255378723\n",
      "cat lion 0.5265437364578247\n",
      "cat cat 1.0\n",
      "cat pet 0.7505456209182739\n",
      "pet lion 0.39923766255378723\n",
      "pet cat 0.7505456209182739\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(t1.text,t2.text,t1.similarity(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54ae3c",
   "metadata": {},
   "source": [
    "Opposites are not necessarily different\n",
    "\n",
    "Words that have opposite meaning, but that often appear in the same context may have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb4841dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.6579039692878723\n",
      "like hate 0.6574652194976807\n",
      "love like 0.6579039692878723\n",
      "love love 1.0\n",
      "love hate 0.6393098831176758\n",
      "hate like 0.6574652194976807\n",
      "hate love 0.6393098831176758\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'like love hate')\n",
    "\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f1c381d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810cc0c",
   "metadata": {},
   "source": [
    "Vector norms\n",
    "\n",
    "It's sometimes helpful to aggregate 300 dimensions into a Euclidian (L2) norm, computed as the square root of the sum-of-squared-vectors. This is accessible as the .vector_norm token attribute. Other helpful attributes include .has_vector and .is_oov or out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c6390596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "hskjdgbfkszdhb False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat hskjdgbfkszdhb')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6cba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459ad547",
   "metadata": {},
   "source": [
    "Vector arithmetic\n",
    "\n",
    "Believe it or not, we can actually calculate new vectors by adding & subtracting related vectors. A famous example suggests\n",
    "\n",
    "\"king\" - \"man\" + \"woman\" = \"queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9983c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing cosine similarity\n",
    "\n",
    "from scipy import spatial\n",
    "cosine_sim=lambda vec1,vec2: 1- spatial.distance.cosine(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbb97552",
   "metadata": {},
   "outputs": [],
   "source": [
    "king=nlp.vocab['king'].vector\n",
    "man=nlp.vocab['man'].vector\n",
    "woman=nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2b1cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector=king-man+woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "961b31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_sim=[]\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity=cosine_sim(new_vector,word.vector)\n",
    "                computed_sim.append((word,similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0bd4bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_sim=sorted(computed_sim,key=lambda item: -item[1])#for descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2bbc22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'woman', 'she', 'lion', 'who', 'fox', 'brown', 'when', 'dare', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print([w[0].text for w in computed_sim[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec64f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e38cdda",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Now that we've seen word vectors we can start to investigate sentiment analysis. The goal is to find commonalities between documents, with the understanding that similarly combined vectors should correspond to similar sentiments.\n",
    "\n",
    "While the scope of sentiment analysis is very broad, we will focus our work in two ways.\n",
    "\n",
    "# Polarity classification\n",
    "\n",
    "We won't try to determine if a sentence is objective or subjective, fact or opinion. Rather, we care only if the text expresses a positive, negative or neutral opinion.\n",
    "\n",
    "# Document level scope\n",
    "\n",
    "We'll also try to aggregate all of the sentences in a document or paragraph, to arrive at an overall opinion.\n",
    "\n",
    "# Coarse analysis\n",
    "\n",
    "We won't try to perform a fine-grained analysis that would determine the degree of positivity/negativity. That is, we're not trying to guess how many stars a reviewer awarded, just whether the review was positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f398dc2",
   "metadata": {},
   "source": [
    "# Broad Steps:\n",
    "\n",
    "First, consider the text being analyzed. A model trained on paragraph-long movie reviews might not be effective on tweets. Make sure to use an appropriate model for the task at hand.\n",
    "Next, decide the type of analysis to perform. In the previous section on text classification we used a bag-of-words technique that considered only single tokens, or unigrams. Some rudimentary sentiment analysis models go one step further, and consider two-word combinations, or bigrams. In this section, we'd like to work with complete sentences, and for this we're going to import a trained NLTK lexicon called VADER.\n",
    "\n",
    "# NLTK's VADER module\n",
    "VADER is an NLTK module that provides sentiment scores based on words used (\"completely\" boosts a score, while \"slightly\" reduces it), on capitalization & punctuation (\"GREAT!!!\" is stronger than \"great.\"), and negations (words like \"isn't\" and \"doesn't\" affect the outcome).\n",
    "To view the source code visit https://www.nltk.org/_modules/nltk/sentiment/vader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aabde953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d97bff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112654e",
   "metadata": {},
   "source": [
    "VADER's SentimentIntensityAnalyzer() takes in a string and returns a dictionary of scores in each of four categories:\n",
    "\n",
    "negative, \n",
    "neutral, \n",
    "positive, \n",
    "compound (computed by normalizing the scores above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1e082bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='This is a good movie'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b18c956e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'This was the best, most awesome movie EVER MADE!!!'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ca454697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.477, 'neu': 0.523, 'pos': 0.0, 'compound': -0.8074}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'This was the worst film to ever disgrace the screen.'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dd3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6c64a8",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba55ae",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3f89b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fbb1a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr=pd.read_csv(\"C:/Users/hp/Desktop/UPDATED_NLP_COURSE/UPDATED_NLP_COURSE/05-Topic-Modeling/npr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "75de300b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  â   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  â   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3198b3",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7fe80b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa0586",
   "metadata": {},
   "source": [
    "max_df: float in range [0.0, 1.0] or int, default=1.0\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df: float in range [0.0, 1.0] or int, default=1\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "143aece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b978ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8050e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "26c51fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "abd0a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=42)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e02a2e",
   "metadata": {},
   "source": [
    "Showing Stored Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c3d99062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "893555cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d3b56471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ef347a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jain\n",
      "shanties\n",
      "fringes\n",
      "themes\n",
      "chieftains\n",
      "subservience\n",
      "cig\n",
      "spinraza\n",
      "webinar\n",
      "interplay\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,54776)\n",
    "    print(cv.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75555ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vietnam\n",
      "oxidized\n",
      "fought\n",
      "metformin\n",
      "literature\n",
      "gorbachev\n",
      "glasse\n",
      "timeline\n",
      "sleeps\n",
      "wallets\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,54776)\n",
    "    print(cv.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203cb90",
   "metadata": {},
   "source": [
    "Showing Top Words Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "475b05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ba4c83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = LDA.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2d2ce0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18302,  2475, 44967, ..., 10425, 42561, 42993], dtype=int64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the indices that would sort this array.\n",
    "single_topic.argsort() #from least to greatest index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "55ac770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000053799"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word least representative of this topic\n",
    "single_topic[18302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "36f10346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1, 18349, 33390, 32089, 10421, 31464, 22673, 10425, 42561,\n",
       "       42993], dtype=int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 greatest indexes of words for this topic:\n",
    "single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "59788eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_indices = single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ed0680ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\n",
      "federal\n",
      "new\n",
      "money\n",
      "companies\n",
      "million\n",
      "health\n",
      "company\n",
      "said\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "for index in top_word_indices:\n",
    "    print(cv.get_feature_names()[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6a9525ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry\n",
      "tax\n",
      "business\n",
      "percent\n",
      "pay\n",
      "people\n",
      "care\n",
      "government\n",
      "year\n",
      "insurance\n",
      "000\n",
      "federal\n",
      "new\n",
      "money\n",
      "companies\n",
      "million\n",
      "health\n",
      "company\n",
      "said\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "top_word_indices = single_topic.argsort()[-20:]\n",
    "for index in top_word_indices:\n",
    "    print(cv.get_feature_names()[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "66428d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "89c230a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['people', 'care', 'government', 'year', 'insurance', '000', 'federal', 'new', 'money', 'companies', 'million', 'health', 'company', 'said', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['npr', 'intelligence', 'security', 'new', 'told', 'russian', 'campaign', 'obama', 'news', 'white', 'russia', 'house', 'president', 'said', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['know', 'little', 'home', 'make', 'way', 'day', 'water', 'time', 'years', 'people', 'food', 'new', 'just', 'like', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['don', 'food', 'work', 'day', 'life', 'time', 'family', 'children', 'years', 'just', 'women', 'world', 'like', 'people', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['supreme', 'order', 'city', 'states', 'federal', 'country', 'president', 'rights', 'government', 'people', 'law', 'state', 'said', 'court', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['going', 've', 'story', 'life', 'don', 'new', 'way', 'time', 'really', 'know', 'think', 'music', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['don', 'student', 'think', 'science', 'university', 'people', 'time', 'schools', 'education', 'just', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['north', 'npr', 'officers', 'china', 'city', 'attack', 'reported', 'killed', 'according', 'told', 'says', 'reports', 'people', 'police', 'said']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #8\n",
      "['like', 'election', 'vote', 'just', 'president', 'voters', 'party', 'state', 'percent', 'republican', 'campaign', 'people', 'said', 'clinton', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #9\n",
      "['doctors', 'university', 'brain', 'like', 'research', 'percent', 'care', 'women', 'medical', 'disease', 'patients', 'study', 'people', 'health', 'says']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "#Probability of words belonging to a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2baa2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a2974941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.78101114e-03, 9.11263140e-01, 1.57269537e-04, 1.57265808e-04,\n",
       "       1.57268730e-04, 1.57266519e-04, 1.57271636e-04, 1.57262374e-04,\n",
       "       7.88549762e-02, 1.57267682e-04])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Probability of a docment belonging to a particular topic\n",
    "topic_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c6a29843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  ])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "19ef66a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0].argmax() #shows highest probaility index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "34d322fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic']=topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8e57d8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  â   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  â   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      6"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6aefc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1326176",
   "metadata": {},
   "source": [
    "# Non-Negative Matric Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "27f08607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8c45d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c624326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c9fcd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c3b1a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "32b9061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Annoconda\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=10, random_state=42)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e3cd177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2cacf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gourmets\n",
      "scheming\n",
      "parochialism\n",
      "unkind\n",
      "silk\n",
      "refinery29\n",
      "infringed\n",
      "prototypes\n",
      "sexless\n",
      "strident\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,54776)\n",
    "    print(tfidf.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a9d2dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = nmf_model.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a076d6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27388, 27031, 27030, ..., 19307, 36283, 42993], dtype=int64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d180433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26752, 10425, 47218, 33390, 36310, 28659, 53152, 19307, 36283,\n",
       "       42993], dtype=int64)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words for this topic:\n",
    "single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "671a19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_indices = single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "78dd3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just\n",
      "company\n",
      "study\n",
      "new\n",
      "percent\n",
      "like\n",
      "water\n",
      "food\n",
      "people\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "for index in top_word_indices:\n",
    "    print(tfidf.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d5e3db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['year', 'university', 'workers', '000', 'years', 'just', 'company', 'study', 'new', 'percent', 'like', 'water', 'food', 'people', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['administration', 'cruz', 'election', 'pence', 'gop', 'presidential', 'obama', 'house', 'white', 'republican', 'donald', 'campaign', 'said', 'president', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['patients', 'repeal', 'law', 'act', 'republicans', 'tax', 'people', 'plan', 'affordable', 'obamacare', 'coverage', 'medicaid', 'insurance', 'care', 'health']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['assad', 'iran', 'iraq', 'north', 'china', 'aleppo', 'war', 'korea', 'said', 'forces', 'russia', 'military', 'syrian', 'syria', 'isis']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['cruz', 'election', 'primary', 'democrats', 'percent', 'party', 'vote', 'state', 'delegates', 'democratic', 'hillary', 'campaign', 'voters', 'sanders', 'clinton']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['book', 'love', 'women', 'way', 'time', 'life', 'album', 'song', 'people', 'really', 'know', 'think', 'just', 'like', 'music']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['program', 'child', 'teacher', 'high', 'devos', 'parents', 'children', 'college', 'kids', 'teachers', 'student', 'education', 'schools', 'school', 'students']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['cdc', 'babies', 'infected', 'brazil', 'outbreak', 'pregnant', 'microcephaly', 'cases', 'health', 'disease', 'mosquitoes', 'mosquito', 'women', 'virus', 'zika']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #8\n",
      "['department', 'enforcement', 'killed', 'city', 'gun', 'people', 'law', 'reports', 'attack', 'shooting', 'officer', 'black', 'said', 'officers', 'police']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #9\n",
      "['law', 'supreme', 'russian', 'russia', 'intelligence', 'said', 'investigation', 'committee', 'house', 'justice', 'president', 'senate', 'fbi', 'court', 'comey']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "13e2274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8c4de05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00591734, 0.94690124, 0.00589809, 0.00589737, 0.00589752,\n",
       "       0.0058973 , 0.0058976 , 0.00589714, 0.005899  , 0.00589739])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b87cf845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.95, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d6eae413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 9, 8, 4], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "99203be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bfadebe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  â   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I did not want to join yoga class. I hated tho...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>With a   who has publicly supported the debunk...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was standing by the airport exit, debating w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If movies were trying to be more realistic, pe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eighteen years ago, on New Yearâs Eve, David F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  â   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2\n",
       "5  I did not want to join yoga class. I hated tho...      9\n",
       "6  With a   who has publicly supported the debunk...      9\n",
       "7  I was standing by the airport exit, debating w...      2\n",
       "8  If movies were trying to be more realistic, pe...      2\n",
       "9  Eighteen years ago, on New Yearâs Eve, David F...      2"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2b33898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_topic={0:'Infrastructure',1:'Politics',2:'Health Care',3:'Military',4:'Election',5:'Music',6:'Education',7:'Infection',8:'Police-Enforcement',9:'Court'}\n",
    "npr['Topic_Name']=npr['Topic'].map(my_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "76d1f86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  â   his prefe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I did not want to join yoga class. I hated tho...</td>\n",
       "      <td>9</td>\n",
       "      <td>Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>With a   who has publicly supported the debunk...</td>\n",
       "      <td>9</td>\n",
       "      <td>Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was standing by the airport exit, debating w...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If movies were trying to be more realistic, pe...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eighteen years ago, on New Yearâs Eve, David F...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>For years now, some of the best, wildest, most...</td>\n",
       "      <td>5</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>For years now, some of the best, wildest, most...</td>\n",
       "      <td>5</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Colorado River is like a giant bank accoun...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>For the last installment of NPRâs holiday reci...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Being overweight can raise your blood pressure...</td>\n",
       "      <td>9</td>\n",
       "      <td>Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Whoâs the YouTube star of 2016? Adele singing ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hereâs a quick roundup of some of the   you ma...</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ben Johnston doesnât follow the rules of music...</td>\n",
       "      <td>5</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>David Bowie, Prince and George Michael are all...</td>\n",
       "      <td>5</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In November, the typically straitlaced Office ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Article  Topic   Topic_Name\n",
       "0   In the Washington of 2016, even when the polic...      1     Politics\n",
       "1     Donald Trump has used Twitter  â   his prefe...      1     Politics\n",
       "2     Donald Trump is unabashedly praising Russian...      1     Politics\n",
       "3   Updated at 2:50 p. m. ET, Russian President Vl...      1     Politics\n",
       "4   From photography, illustration and video, to d...      2  Health Care\n",
       "5   I did not want to join yoga class. I hated tho...      9        Court\n",
       "6   With a   who has publicly supported the debunk...      9        Court\n",
       "7   I was standing by the airport exit, debating w...      2  Health Care\n",
       "8   If movies were trying to be more realistic, pe...      2  Health Care\n",
       "9   Eighteen years ago, on New Yearâs Eve, David F...      2  Health Care\n",
       "10  For years now, some of the best, wildest, most...      5        Music\n",
       "11  For years now, some of the best, wildest, most...      5        Music\n",
       "12  The Colorado River is like a giant bank accoun...      2  Health Care\n",
       "13  For the last installment of NPRâs holiday reci...      2  Health Care\n",
       "14  Being overweight can raise your blood pressure...      9        Court\n",
       "15  Whoâs the YouTube star of 2016? Adele singing ...      5        Music\n",
       "16  Hereâs a quick roundup of some of the   you ma...      2  Health Care\n",
       "17  Ben Johnston doesnât follow the rules of music...      5        Music\n",
       "18  David Bowie, Prince and George Michael are all...      5        Music\n",
       "19  In November, the typically straitlaced Office ...      1     Politics"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd58a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed336aab",
   "metadata": {},
   "source": [
    "# Keras Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "56905a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0ad36305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4a700e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8b928616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ffd4d700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "82ba2951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "657777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "72cb31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b6f8e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1133b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a6c04f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d97796b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d3cc12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bc043fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9564987",
   "metadata": {},
   "source": [
    "Standardizing the Data\n",
    "\n",
    "Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9ccb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a2c7722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "da84a3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_object.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1c46e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "aaca4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ecd6",
   "metadata": {},
   "source": [
    "# Building the Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "df4cb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4d7e13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1b16c737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "99e2c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 - 2s - loss: 1.1175 - accuracy: 0.3400 - 2s/epoch - 460ms/step\n",
      "Epoch 2/150\n",
      "4/4 - 0s - loss: 1.1136 - accuracy: 0.3400 - 7ms/epoch - 2ms/step\n",
      "Epoch 3/150\n",
      "4/4 - 0s - loss: 1.1101 - accuracy: 0.3500 - 6ms/epoch - 2ms/step\n",
      "Epoch 4/150\n",
      "4/4 - 0s - loss: 1.1071 - accuracy: 0.3500 - 6ms/epoch - 1ms/step\n",
      "Epoch 5/150\n",
      "4/4 - 0s - loss: 1.1036 - accuracy: 0.3700 - 5ms/epoch - 1ms/step\n",
      "Epoch 6/150\n",
      "4/4 - 0s - loss: 1.1004 - accuracy: 0.4100 - 5ms/epoch - 1ms/step\n",
      "Epoch 7/150\n",
      "4/4 - 0s - loss: 1.0968 - accuracy: 0.4300 - 5ms/epoch - 1ms/step\n",
      "Epoch 8/150\n",
      "4/4 - 0s - loss: 1.0932 - accuracy: 0.4700 - 5ms/epoch - 1ms/step\n",
      "Epoch 9/150\n",
      "4/4 - 0s - loss: 1.0893 - accuracy: 0.5100 - 6ms/epoch - 1ms/step\n",
      "Epoch 10/150\n",
      "4/4 - 0s - loss: 1.0851 - accuracy: 0.5700 - 5ms/epoch - 1ms/step\n",
      "Epoch 11/150\n",
      "4/4 - 0s - loss: 1.0809 - accuracy: 0.6300 - 4ms/epoch - 1ms/step\n",
      "Epoch 12/150\n",
      "4/4 - 0s - loss: 1.0766 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 13/150\n",
      "4/4 - 0s - loss: 1.0724 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 14/150\n",
      "4/4 - 0s - loss: 1.0680 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 15/150\n",
      "4/4 - 0s - loss: 1.0639 - accuracy: 0.6600 - 6ms/epoch - 2ms/step\n",
      "Epoch 16/150\n",
      "4/4 - 0s - loss: 1.0595 - accuracy: 0.6600 - 6ms/epoch - 1ms/step\n",
      "Epoch 17/150\n",
      "4/4 - 0s - loss: 1.0550 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 18/150\n",
      "4/4 - 0s - loss: 1.0507 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 19/150\n",
      "4/4 - 0s - loss: 1.0460 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 20/150\n",
      "4/4 - 0s - loss: 1.0416 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 21/150\n",
      "4/4 - 0s - loss: 1.0369 - accuracy: 0.6600 - 6ms/epoch - 1ms/step\n",
      "Epoch 22/150\n",
      "4/4 - 0s - loss: 1.0323 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 23/150\n",
      "4/4 - 0s - loss: 1.0278 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 24/150\n",
      "4/4 - 0s - loss: 1.0229 - accuracy: 0.6600 - 6ms/epoch - 2ms/step\n",
      "Epoch 25/150\n",
      "4/4 - 0s - loss: 1.0183 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 26/150\n",
      "4/4 - 0s - loss: 1.0138 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 27/150\n",
      "4/4 - 0s - loss: 1.0092 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 28/150\n",
      "4/4 - 0s - loss: 1.0045 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 29/150\n",
      "4/4 - 0s - loss: 0.9998 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 30/150\n",
      "4/4 - 0s - loss: 0.9952 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 31/150\n",
      "4/4 - 0s - loss: 0.9906 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 32/150\n",
      "4/4 - 0s - loss: 0.9858 - accuracy: 0.6600 - 6ms/epoch - 2ms/step\n",
      "Epoch 33/150\n",
      "4/4 - 0s - loss: 0.9811 - accuracy: 0.6600 - 6ms/epoch - 1ms/step\n",
      "Epoch 34/150\n",
      "4/4 - 0s - loss: 0.9763 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 35/150\n",
      "4/4 - 0s - loss: 0.9713 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 36/150\n",
      "4/4 - 0s - loss: 0.9662 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 37/150\n",
      "4/4 - 0s - loss: 0.9605 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 38/150\n",
      "4/4 - 0s - loss: 0.9544 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 39/150\n",
      "4/4 - 0s - loss: 0.9480 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 40/150\n",
      "4/4 - 0s - loss: 0.9414 - accuracy: 0.6600 - 4ms/epoch - 997us/step\n",
      "Epoch 41/150\n",
      "4/4 - 0s - loss: 0.9347 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 42/150\n",
      "4/4 - 0s - loss: 0.9279 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 43/150\n",
      "4/4 - 0s - loss: 0.9208 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 44/150\n",
      "4/4 - 0s - loss: 0.9135 - accuracy: 0.6600 - 4ms/epoch - 997us/step\n",
      "Epoch 45/150\n",
      "4/4 - 0s - loss: 0.9060 - accuracy: 0.6600 - 6ms/epoch - 2ms/step\n",
      "Epoch 46/150\n",
      "4/4 - 0s - loss: 0.8986 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 47/150\n",
      "4/4 - 0s - loss: 0.8910 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 48/150\n",
      "4/4 - 0s - loss: 0.8836 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 49/150\n",
      "4/4 - 0s - loss: 0.8762 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 50/150\n",
      "4/4 - 0s - loss: 0.8684 - accuracy: 0.6600 - 6ms/epoch - 1ms/step\n",
      "Epoch 51/150\n",
      "4/4 - 0s - loss: 0.8610 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 52/150\n",
      "4/4 - 0s - loss: 0.8533 - accuracy: 0.7000 - 5ms/epoch - 1ms/step\n",
      "Epoch 53/150\n",
      "4/4 - 0s - loss: 0.8457 - accuracy: 0.7500 - 5ms/epoch - 1ms/step\n",
      "Epoch 54/150\n",
      "4/4 - 0s - loss: 0.8382 - accuracy: 0.8100 - 5ms/epoch - 1ms/step\n",
      "Epoch 55/150\n",
      "4/4 - 0s - loss: 0.8307 - accuracy: 0.9100 - 5ms/epoch - 1ms/step\n",
      "Epoch 56/150\n",
      "4/4 - 0s - loss: 0.8230 - accuracy: 0.8800 - 4ms/epoch - 1ms/step\n",
      "Epoch 57/150\n",
      "4/4 - 0s - loss: 0.8153 - accuracy: 0.8200 - 6ms/epoch - 1ms/step\n",
      "Epoch 58/150\n",
      "4/4 - 0s - loss: 0.8080 - accuracy: 0.7600 - 3ms/epoch - 846us/step\n",
      "Epoch 59/150\n",
      "4/4 - 0s - loss: 0.8004 - accuracy: 0.7300 - 6ms/epoch - 2ms/step\n",
      "Epoch 60/150\n",
      "4/4 - 0s - loss: 0.7932 - accuracy: 0.7700 - 7ms/epoch - 2ms/step\n",
      "Epoch 61/150\n",
      "4/4 - 0s - loss: 0.7860 - accuracy: 0.7900 - 5ms/epoch - 1ms/step\n",
      "Epoch 62/150\n",
      "4/4 - 0s - loss: 0.7786 - accuracy: 0.7900 - 5ms/epoch - 1ms/step\n",
      "Epoch 63/150\n",
      "4/4 - 0s - loss: 0.7714 - accuracy: 0.7800 - 5ms/epoch - 1ms/step\n",
      "Epoch 64/150\n",
      "4/4 - 0s - loss: 0.7641 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 65/150\n",
      "4/4 - 0s - loss: 0.7569 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 66/150\n",
      "4/4 - 0s - loss: 0.7494 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 67/150\n",
      "4/4 - 0s - loss: 0.7425 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 68/150\n",
      "4/4 - 0s - loss: 0.7355 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 69/150\n",
      "4/4 - 0s - loss: 0.7289 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 70/150\n",
      "4/4 - 0s - loss: 0.7224 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 71/150\n",
      "4/4 - 0s - loss: 0.7156 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 72/150\n",
      "4/4 - 0s - loss: 0.7092 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 73/150\n",
      "4/4 - 0s - loss: 0.7026 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 74/150\n",
      "4/4 - 0s - loss: 0.6961 - accuracy: 0.6500 - 4ms/epoch - 976us/step\n",
      "Epoch 75/150\n",
      "4/4 - 0s - loss: 0.6893 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 76/150\n",
      "4/4 - 0s - loss: 0.6830 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 77/150\n",
      "4/4 - 0s - loss: 0.6765 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 78/150\n",
      "4/4 - 0s - loss: 0.6704 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 79/150\n",
      "4/4 - 0s - loss: 0.6642 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 80/150\n",
      "4/4 - 0s - loss: 0.6583 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 81/150\n",
      "4/4 - 0s - loss: 0.6526 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 82/150\n",
      "4/4 - 0s - loss: 0.6471 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 83/150\n",
      "4/4 - 0s - loss: 0.6418 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 84/150\n",
      "4/4 - 0s - loss: 0.6363 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 85/150\n",
      "4/4 - 0s - loss: 0.6311 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 86/150\n",
      "4/4 - 0s - loss: 0.6260 - accuracy: 0.6500 - 4ms/epoch - 1ms/step\n",
      "Epoch 87/150\n",
      "4/4 - 0s - loss: 0.6211 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 88/150\n",
      "4/4 - 0s - loss: 0.6164 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 89/150\n",
      "4/4 - 0s - loss: 0.6115 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 90/150\n",
      "4/4 - 0s - loss: 0.6067 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 91/150\n",
      "4/4 - 0s - loss: 0.6020 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 92/150\n",
      "4/4 - 0s - loss: 0.5975 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 93/150\n",
      "4/4 - 0s - loss: 0.5931 - accuracy: 0.6500 - 4ms/epoch - 1ms/step\n",
      "Epoch 94/150\n",
      "4/4 - 0s - loss: 0.5892 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 95/150\n",
      "4/4 - 0s - loss: 0.5850 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 96/150\n",
      "4/4 - 0s - loss: 0.5809 - accuracy: 0.6500 - 4ms/epoch - 1ms/step\n",
      "Epoch 97/150\n",
      "4/4 - 0s - loss: 0.5771 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 98/150\n",
      "4/4 - 0s - loss: 0.5733 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 99/150\n",
      "4/4 - 0s - loss: 0.5694 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 100/150\n",
      "4/4 - 0s - loss: 0.5660 - accuracy: 0.6500 - 3ms/epoch - 866us/step\n",
      "Epoch 101/150\n",
      "4/4 - 0s - loss: 0.5623 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 102/150\n",
      "4/4 - 0s - loss: 0.5588 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 103/150\n",
      "4/4 - 0s - loss: 0.5553 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 104/150\n",
      "4/4 - 0s - loss: 0.5519 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 105/150\n",
      "4/4 - 0s - loss: 0.5486 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 106/150\n",
      "4/4 - 0s - loss: 0.5454 - accuracy: 0.6500 - 4ms/epoch - 1ms/step\n",
      "Epoch 107/150\n",
      "4/4 - 0s - loss: 0.5423 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 108/150\n",
      "4/4 - 0s - loss: 0.5392 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 109/150\n",
      "4/4 - 0s - loss: 0.5363 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 110/150\n",
      "4/4 - 0s - loss: 0.5332 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 111/150\n",
      "4/4 - 0s - loss: 0.5302 - accuracy: 0.6500 - 5ms/epoch - 1ms/step\n",
      "Epoch 112/150\n",
      "4/4 - 0s - loss: 0.5272 - accuracy: 0.6500 - 9ms/epoch - 2ms/step\n",
      "Epoch 113/150\n",
      "4/4 - 0s - loss: 0.5245 - accuracy: 0.6500 - 3ms/epoch - 721us/step\n",
      "Epoch 114/150\n",
      "4/4 - 0s - loss: 0.5217 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 115/150\n",
      "4/4 - 0s - loss: 0.5190 - accuracy: 0.6500 - 4ms/epoch - 1ms/step\n",
      "Epoch 116/150\n",
      "4/4 - 0s - loss: 0.5164 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 117/150\n",
      "4/4 - 0s - loss: 0.5139 - accuracy: 0.6600 - 5ms/epoch - 1ms/step\n",
      "Epoch 118/150\n",
      "4/4 - 0s - loss: 0.5114 - accuracy: 0.6700 - 7ms/epoch - 2ms/step\n",
      "Epoch 119/150\n",
      "4/4 - 0s - loss: 0.5089 - accuracy: 0.6700 - 4ms/epoch - 999us/step\n",
      "Epoch 120/150\n",
      "4/4 - 0s - loss: 0.5065 - accuracy: 0.6700 - 6ms/epoch - 1ms/step\n",
      "Epoch 121/150\n",
      "4/4 - 0s - loss: 0.5039 - accuracy: 0.6700 - 7ms/epoch - 2ms/step\n",
      "Epoch 122/150\n",
      "4/4 - 0s - loss: 0.5016 - accuracy: 0.6800 - 6ms/epoch - 1ms/step\n",
      "Epoch 123/150\n",
      "4/4 - 0s - loss: 0.4992 - accuracy: 0.7000 - 5ms/epoch - 1ms/step\n",
      "Epoch 124/150\n",
      "4/4 - 0s - loss: 0.4968 - accuracy: 0.7000 - 7ms/epoch - 2ms/step\n",
      "Epoch 125/150\n",
      "4/4 - 0s - loss: 0.4947 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 126/150\n",
      "4/4 - 0s - loss: 0.4922 - accuracy: 0.7200 - 5ms/epoch - 1ms/step\n",
      "Epoch 127/150\n",
      "4/4 - 0s - loss: 0.4900 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 128/150\n",
      "4/4 - 0s - loss: 0.4880 - accuracy: 0.7200 - 5ms/epoch - 1ms/step\n",
      "Epoch 129/150\n",
      "4/4 - 0s - loss: 0.4855 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 130/150\n",
      "4/4 - 0s - loss: 0.4836 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 131/150\n",
      "4/4 - 0s - loss: 0.4814 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 132/150\n",
      "4/4 - 0s - loss: 0.4792 - accuracy: 0.7300 - 5ms/epoch - 1ms/step\n",
      "Epoch 133/150\n",
      "4/4 - 0s - loss: 0.4778 - accuracy: 0.7300 - 7ms/epoch - 2ms/step\n",
      "Epoch 134/150\n",
      "4/4 - 0s - loss: 0.4752 - accuracy: 0.7300 - 4ms/epoch - 965us/step\n",
      "Epoch 135/150\n",
      "4/4 - 0s - loss: 0.4728 - accuracy: 0.7300 - 6ms/epoch - 1ms/step\n",
      "Epoch 136/150\n",
      "4/4 - 0s - loss: 0.4706 - accuracy: 0.7300 - 6ms/epoch - 1ms/step\n",
      "Epoch 137/150\n",
      "4/4 - 0s - loss: 0.4695 - accuracy: 0.7200 - 6ms/epoch - 2ms/step\n",
      "Epoch 138/150\n",
      "4/4 - 0s - loss: 0.4676 - accuracy: 0.7200 - 7ms/epoch - 2ms/step\n",
      "Epoch 139/150\n",
      "4/4 - 0s - loss: 0.4658 - accuracy: 0.7200 - 4ms/epoch - 950us/step\n",
      "Epoch 140/150\n",
      "4/4 - 0s - loss: 0.4638 - accuracy: 0.7200 - 6ms/epoch - 2ms/step\n",
      "Epoch 141/150\n",
      "4/4 - 0s - loss: 0.4619 - accuracy: 0.7200 - 7ms/epoch - 2ms/step\n",
      "Epoch 142/150\n",
      "4/4 - 0s - loss: 0.4604 - accuracy: 0.7200 - 6ms/epoch - 1ms/step\n",
      "Epoch 143/150\n",
      "4/4 - 0s - loss: 0.4585 - accuracy: 0.7200 - 6ms/epoch - 2ms/step\n",
      "Epoch 144/150\n",
      "4/4 - 0s - loss: 0.4567 - accuracy: 0.7200 - 5ms/epoch - 1ms/step\n",
      "Epoch 145/150\n",
      "4/4 - 0s - loss: 0.4550 - accuracy: 0.7200 - 5ms/epoch - 1ms/step\n",
      "Epoch 146/150\n",
      "4/4 - 0s - loss: 0.4538 - accuracy: 0.7200 - 6ms/epoch - 2ms/step\n",
      "Epoch 147/150\n",
      "4/4 - 0s - loss: 0.4524 - accuracy: 0.7200 - 12ms/epoch - 3ms/step\n",
      "Epoch 148/150\n",
      "4/4 - 0s - loss: 0.4506 - accuracy: 0.7200 - 12ms/epoch - 3ms/step\n",
      "Epoch 149/150\n",
      "4/4 - 0s - loss: 0.4484 - accuracy: 0.7300 - 12ms/epoch - 3ms/step\n",
      "Epoch 150/150\n",
      "4/4 - 0s - loss: 0.4464 - accuracy: 0.7300 - 12ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23503ec5b50>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X_train,y_train,epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2ca20b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7221172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0,\n",
       "       0, 1, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_x=model.predict(scaled_X_test) \n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "classes_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4decb",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7d09e596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f918bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3939 - accuracy: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3938922584056854, 0.7799999713897705]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaled_X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "164da145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5c536a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e1f67502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0,  4, 11],\n",
       "       [ 0,  0, 16]], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1),classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c5329720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.27      0.42        15\n",
      "           2       0.59      1.00      0.74        16\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.86      0.76      0.72        50\n",
      "weighted avg       0.87      0.78      0.74        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),classes_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a9be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
